{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kræsjkurs i AI - Grunnleggende prinsipper\n",
    "\n",
    "## Kunstig intelligens er så mangt\n",
    "<img src=\"https://media.licdn.com/dms/image/D5612AQE4PAb7ReqhRg/article-cover_image-shrink_600_2000/0/1679715986172?e=2147483647&v=beta&t=LlQEO0Tf583irseJlj3G3BI2pvpJaLyGM_gxALuwB0E\">\n",
    "\n",
    "Venn-diagrammet viser at tre buzzwords vi er kjent med i dagligtalen faller inn under hverandre.\n",
    "- AI\n",
    "    - Ethvert system som utfører en oppgave hvor menneskelig intelligens kreves faller under begrepet AI.\n",
    "    - Eksempler\n",
    "        - Søkealgoritmer som A*\n",
    "        - Ekspertsystemer / regel-baserte systemer\n",
    "            - Består av regler som er lagd av mennesker\n",
    "        - Alt som faller inn under maskinlæring\n",
    "- Maskinlæring\n",
    "    - Et system/algoritme som bygger opp en form for kunnskap basert på data. Denne kunnskapen brukes så til å løse problemer. \n",
    "    - Implementasjonen består av to faser \n",
    "        - Trening\n",
    "        - Bruk av modellen (også kalt inference)\n",
    "    - Eksempler\n",
    "        - Beslutningstrær\n",
    "            - <img src=\"https://images.datacamp.com/image/upload/v1677504957/decision_tree_for_heart_attack_prevention_2140bd762d.png\" height=\"300px\">\n",
    "            Reglene konstrueres ved hjelp av en treningsalgoritme\n",
    "        - Alt som faller inn under dyp læring\n",
    "- Dyp læring\n",
    "    - Et system som bygger opp en form for kunnskap basert på data ved bruk av nevrale nettverk.\n",
    "    - Eksempler\n",
    "        - Blir gjennomgått i kodebiten av denne notebooken\n",
    "\n",
    "Det finnes en annen inndelig av AI som også brukes ofte:\n",
    "- Symbolsk AI\n",
    "    - AI hvor beslutningene skjer på grunnlag av regler som kan deles opp i _symboler_. \n",
    "    - Eksempler\n",
    "        - Regler som spesifiseres ved hjelp av et programmeringsspråk, altså en algoritme.\n",
    "- Subsymbolsk AI\n",
    "    - AI hvor beslutningene skjer på grunnlag av regler som ikke kan direkte tolkes av mennesker\n",
    "    - Er mer matematisk jordet. \n",
    "    - Eksklusivt dyp læring, altså bare den innerste gruppen i venn-diagrammet.\n",
    "\n",
    "    \n",
    "## Dyp læring\n",
    "\n",
    "Venn-diagrammet er litt utdatert. Under _Deep Learning_ mangler det feks:\n",
    "- Transformers\n",
    "    - En arkitektur hovedsaklig for språkforståelse vi **garantert** kommer til å ta i bruk.\n",
    "- Diffusjonsmodeller\n",
    "    - En modell som de siste to årene har oppnåd bedre resultater enn Generative Adversarial Networks (GAN). Brukes for det meste på bildesyntese.\n",
    "\n",
    "- Hvorfor bryr vi oss mest om dyp læring nå (både i dette kurset og til daglig)?\n",
    "    - Flesteparten av modellene som imponerer oss består av dype nevrale nettverk.\n",
    "        - ChatGPT\n",
    "        - GPT-4\n",
    "        - Stable Diffusion\n",
    "        - Midjourney\n",
    "        - AlphaZero\n",
    "    - Tekniske ressurser er tilgjengelige for å realisere mye av teorien som ble funnet på 50-tallet.\n",
    "\n",
    "## Former for maskinlæring\n",
    "- Supervised learning\n",
    "    - Man bruker data og beskrivelser av dataen til å trene opp en modell.\n",
    "- Unsupervised learning\n",
    "    - Man bruker bare selve dataen, og modellen finner mønstre som man etterpå kan studere.\n",
    "- Reinforcement learning\n",
    "    - Man plasserer en modell i et miljø som den får utforske og samle inn data selv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Den tekniske biten - Hvordan ser det ut å jobbe med nevrale nettverk nå til dags?\n",
    "Denne er ment for å gi et kjapt overblikk av hvordan kode ser ut når man jobber med maskinlæring / dyp læring.\n",
    "\n",
    "Intrukser for å kjøre kodeceller:\n",
    "- VSCode og Google Colab\n",
    "    - Velg en celle og trykk på ▶️-knappen til venstre av cellen\n",
    "- Jupyter Notebook i nettleseren\n",
    "    - Velg en celle og trykk på ▶️-knappen i navigasjonsmenyen i toppen av vinduet. \n",
    "- Shortcut for alle nevnt over\n",
    "    - Velg en celle og trykk `CTRL-Enter`\n",
    "\n",
    "\n",
    "\n",
    "## Vektorer\n",
    "I maskinlæring er all data i form av vektorer.\n",
    "\n",
    "Vi begynner med Numpy, som er det mest brukte biblioteket for data science i Python. Biblioteket tilbyr kraftigere metoder for å håndtere native Python-lister."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensjoner og _shapes_\n",
    "Fra matematikken vet vi at en liste med feks. 3 tall definerer en vektor/punkt som lever i et tre-dimensjonalt rom. Dette ser slikt ut i Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [3, 2, 1] # Starting with a normal python list\n",
    "vector = np.array(data) # Creating a numpy array from it\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med Numpy-arrays er det flere operasjoner som kan gjøres (som vi også til en grad vil forvente av native Python-arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value: 3\n",
      "Min value: 1\n",
      "Index of max element: 0\n",
      "Index of max element: 2\n",
      "Sum: 6\n",
      "Sorted:  [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max value: {vector.max()}\")\n",
    "print(f\"Min value: {vector.min()}\")\n",
    "\n",
    "print(f\"Index of max element: {vector.argmax()}\")\n",
    "print(f\"Index of max element: {vector.argmin()}\")\n",
    "\n",
    "print(f\"Sum: {vector.sum()}\")\n",
    "\n",
    "print(f\"Sorted:  {np.sort(vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "På lik linje har vi et 16-dimensjonalt punkt (som vi heller lager med tilfeldige tall):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 9, 4, 4, 1, 8, 6, 4, 7, 1, 2, 1, 5, 9, 8, 8])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_dim_vector = np.random.randint(low=1, high=10, size=(16))\n",
    "high_dim_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matematikken sier videre at en matrise er en rektangulær liste / tabell. For matriser definerer man antall rader og kolonner. Det er her Numpy begynner å divergere litt fra vanlige Python-arrays. \n",
    "\n",
    "Vi lager en $2 \\times 3$-matrise, altså med 2 rader og 3 kolonner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 3, 5],\n",
       "       [3, 8, 8]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = np.random.randint(low=1, high=10, size=(2, 3))\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er vi usikker på hvilken _shape_ matrisen har, kan vi sjekke det med `.shape` attributtet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the matrix is (2, 3)\n",
      "It has 2 rows and 3 columns\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the matrix is {matrix.shape}\")\n",
    "rows, cols = matrix.shape\n",
    "print(f\"It has {rows} rows and {cols} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan gå videre ved å legge til dybde på en matrise, og lage det som kalles en tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the tensor is (2, 3, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[5, 8, 1, 9],\n",
       "        [9, 6, 4, 7],\n",
       "        [8, 0, 4, 5]],\n",
       "\n",
       "       [[2, 7, 8, 2],\n",
       "        [9, 4, 1, 5],\n",
       "        [3, 7, 6, 9]]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = np.random.randint(low=0, high=10, size=(2, 3, 4))\n",
    "print(f\"The shape of the tensor is {tensor.shape}\")\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensoren over består av 2 matriser som hver har 3 rader og 4 koloner.\n",
    "\n",
    "Min faglige erfaring tilsier at særlig begrepene _vektor_ og _tensor_ er generelle, og beskriver bare en samling verdier strukturert med en vilkårlig _shape_. Rent praktisk er de flerdimensjonale lister/arrays. I denne gjennomgangen vil begge begrepene brukes litt her og der.\n",
    "\n",
    "Her kommer det derfor et veiskille i hva man legger i begrepet _dimensjoner_ når man referer til en tensor:\n",
    "- Antall elementer til sammen i tensoren (matematisk riktig)\n",
    "- Antall elementer som ligger i resultatet av `.shape`-attributtet \n",
    "\n",
    "For å redusere forvirring kaller man ofte den første for antall _features_. Den andre kan vi kalle for shape-dimensjoner. \n",
    "\n",
    "Numpy-arrays er veldig fleksible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the reshaped tensor: (2, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9, 7, 9, 0, 2, 7, 1, 4, 4, 8, 9, 3],\n",
       "       [7, 9, 7, 8, 3, 9, 1, 1, 9, 2, 8, 0]])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped = tensor.reshape(2, 3*4) # Combine the rows and columns into one\n",
    "print(f\"Shape of the reshaped tensor: {reshaped.shape}\")\n",
    "reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis det er en shape-dimensjon vi ikke har styr på og ofte varierer kan vi la Numpy regne det ut selv ved å spesifisere `-1` for **en** av posisjonene.\n",
    "\n",
    "Dette er veldig nyttig når vi ønsker å samle flere datapunkt i en tensor. Vi vet gjerne hvordan et datapunkt ser ut, feks et bilde vil ha en bestemt høyde og bredde og antall fargekanaler. Og disse verdiene vil være felles for alle datapunkt i datasettet. Denne prosessen kalles for batching, og vi kommer tilbake til det senere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 8, 1, 9, 9, 6, 4, 7, 8, 0, 4, 5],\n",
       "       [2, 7, 8, 2, 9, 4, 1, 5, 3, 7, 6, 9]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped = tensor.reshape(-1, 3*4)\n",
    "reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når vi har flere _shape_-dimensjoner har vi muligheten til å spesifisere dimensjoner av interesse for noen av operasjonene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 6, 9],\n",
       "       [3, 7, 7]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = np.random.randint(low=0, high=10, size=(2, 3))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summing along rows: [ 4 13 16]\n",
      "Summing along columns: [16 17]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Summing along rows: {tensor.sum(axis=0)}\")\n",
    "print(f\"Summing along columns: {tensor.sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min values along rows: [1 6 7]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Min values along rows: {tensor.min(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rammeverket PyTorch\n",
    "\n",
    "PyTorch er et av flere biblioteker som brukes til trening av dype nevrale nettverk. Det andre kjente alternativet er TensorFlow. Hva man velger avhenger mye av smak, men begge har sine fordeler og ulemper. Jeg liker PyTorch fordi mindre av treningsprosessen blir gjemt og man har mer frihet.\n",
    "PyTorch omfatter mye av den funksjonaliteten man finner i Numpy, men tilbyr metoder for trening og konstruksjon av nevrale nettverk. En `np.ndarray` er ekvivalent med en `torch.Tensor`. Derfor tar vi nå med oss det vi har lært om Numpy-arrays når vi studerer Pytorch-tensorer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0066, 0.2704],\n",
      "        [0.9237, 0.1901]])\n",
      "Tensor shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(2, 2)\n",
    "print(tensor)\n",
    "print(f\"Tensor shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er to viktige funksjonaliteter `torch.Tensor` implementerer, som man ikke finner i Numpy-arrays:\n",
    "1. Støtte for å flytte dataene mellom CPU og GPU (hvis installert på datamaskinen).\n",
    "2. Loggføring av hvilke operasjoner som er gjort på tensoren (nyttiggjøres under trening)\n",
    "\n",
    "Vi ser på det første punktet ved å lage en tensor og sjekke hvilken enhet den havner på."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default tensor location: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Default tensor location: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0066, 0.2704],\n",
       "        [0.9237, 0.1901]], device='cuda:0')"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det andre punktet er loggføring av operasjoner på tensorer. Dette skjer bare på tensorer hvor flagget `requires_grad` er aktivert, eller som deltar i en operasjon med en annen tensor som har det aktivert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1480, 0.7233],\n",
       "        [0.7188, 0.1677]], requires_grad=True)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(2, 2, requires_grad=True)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1480, 2.7233],\n",
       "        [2.7188, 2.1677]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor + 2\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21.4804, 27.2333],\n",
       "        [27.1883, 21.6771]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor * 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1480, 2.7233],\n",
       "        [2.7188, 2.1677]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor / 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan sjekke loggen for å se at operasjonene er dokumentert. \n",
    "\n",
    "**Merk**: Dette gjør vi for moro skyld. Det er ikke noe man gjør i praksis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DivBackward0 object at 0x000002894A6F2380>\n",
      "<MulBackward0 object at 0x00000288B0C72860>\n",
      "<AddBackward0 object at 0x000002894A6F2380>\n"
     ]
    }
   ],
   "source": [
    "grad_fn = tensor.grad_fn\n",
    "while len(grad_fn.next_functions) != 0:\n",
    "    print(grad_fn)\n",
    "    grad_fn = grad_fn.next_functions[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppsett av nevrale nettverk med PyTorch\n",
    "Nevrale nettverk har vanligvis komplekse strukturer. Likevel består de av flere isolerte komponenter, og disse finner man i `torch.nn`. \n",
    "\n",
    "De fleste komponentene blir et såkalt _lag_ i nettverket, mens andre komponenter anvendes på eksisterende lag (feks aktiveringsfunksjoner). \n",
    "\n",
    "Den enkleste er `torch.nn.Linear`, og setter opp et lineært lag. Matematisk gjør den en lineær transformasjon fra et vektorrom til et annet. Feks kan den ta inn en vektor med 5 features, og outputte en vektor med 3 features:\n",
    "\n",
    "<img src=\"nn_5in_3out-cropped.svg\" width=\"500px\" height=\"auto\" alt=\"SVG Image\"  style=\"filter: invert(100%); \"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=3, bias=True)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "layer = nn.Linear(in_features=5, out_features=3) # Construct layer\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector: tensor([[1., 2., 3., 4., 5.]])\n",
      "Output vector: tensor([[-0.1922, -4.7326, -0.2787]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data = torch.Tensor([[1, 2, 3, 4, 5]])\n",
    "print(f\"Input vector: {data}\")\n",
    "output = layer(data) # Feed data into layer\n",
    "print(f\"Output vector: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0853, -0.3824,  0.3510, -0.2863,  0.1341],\n",
      "        [-0.4117, -0.2032, -0.4175, -0.2306, -0.2884],\n",
      "        [-0.0813,  0.3443,  0.2360, -0.0171, -0.2706]], requires_grad=True)\n",
      "Weight shape: torch.Size([3, 5])\n",
      "\n",
      "Parameter containing:\n",
      "tensor([-0.0910, -0.2974, -0.1727], requires_grad=True)\n",
      "Bias shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(layer.weight)\n",
    "print(f\"Weight shape: {layer.weight.shape}\\n\")\n",
    "\n",
    "print(layer.bias)\n",
    "print(f\"Bias shape: {layer.bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når man sender input-vektoren inn skjer følgende operasjon (hvor `@` er matrisemultiplikasjon):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1922, -4.7326, -0.2787]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data @ layer.weight.T + layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ser fra resultatet over at det stemmer .\n",
    "\n",
    "\n",
    "Vektene og biaset utgjør til sammen **parametrene** for dette lineære laget, og er verdiene som endres under trening. Hvordan disse er strukturert og brukes varierer for andre typer lag. Konvolusjonelle lag bruker en \"sliding window\"-mekanisme hvor de samme vektene multipliseres med forskjellige deler av et bilde. Da brukes vanligvis et mindre antall parametre. Til denne introduksjon holder vi oss derimot til lineære lag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktiveringsfunksjoner\n",
    "\n",
    "Det lineære laget utfører en lineær operasjon. Fleksibiliteten av nevrale nettverk kommer derimot av såkalte _aktiveringsfunksjoner_ som utfører ikke-lineære operasjoner på data. Disse inneholder **vanligvis ikke** trenbare parametre. De enkleste opererer på hvert element individuelt, som feks Tanh. Vi ser på et en-dimensjonalt-case for å visualisere det enkelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt # Library for visualization\n",
    "tanh = nn.Tanh()\n",
    "assert not hasattr(tanh, \"weight\") # No learnable weights\n",
    "\n",
    "data = torch.linspace(start=-6, end=6, steps=100) # One-dimensional vector with 100 elements \n",
    "output = tanh(data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "\n",
    "ax.plot(data, output)\n",
    "ax.set_xlabel(\"Input data\")\n",
    "ax.set_ylabel(\"Tanh Output\")\n",
    "\n",
    "# Add a grid\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et viktig poeng er at aktiveringsfunksjoner ikke modifiserer _shapen_ til tensoren. Dette kan demonstreres på en tensor med flere shape-dimensjoner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before activation function: torch.Size([30, 5, 15])\n",
      "shape after activation function: torch.Size([30, 5, 15])\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(30, 5, 15)\n",
    "print(f\"shape before activation function: {data.shape}\")\n",
    "data = tanh(data)\n",
    "shape2 = print(f\"shape after activation function: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En annen viktig er `nn.Softmax`, og brukes til å transformere en vektor til elementer som summeres til 1. \n",
    "\n",
    "Dette gjør den litt annerledes enn Tanh, siden vi må spesifisere en av shape-dimensjonene som skal summeres til 1.\n",
    "\n",
    "Passer fint som siste lag i et nettverk man ønsker skal modellere en sannsynlighetsfordeling (Total sannsynlighet av alle utfallene av en stokastisk variabel skal være 1). Derfor er den veldig relevant for NLP, siden de fleste språkmodeller lærer seg en betinget sannsynlighetsfordeling over ord gitt tidligere tekst. \n",
    "\n",
    "For denne anledningen tar vi også i bruk `nn.functional`, som er et delbibliotek som tilbyr mange av komponentene tilstandsfrie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "tensor([[ 0.9102,  0.2943, -0.0205],\n",
      "        [ 0.0271, -0.5908,  0.4261],\n",
      "        [ 0.2740, -1.8765,  1.0024]])\n",
      "Transformed data:\n",
      "tensor([[0.5169, 0.2792, 0.2038],\n",
      "        [0.3301, 0.1779, 0.4920],\n",
      "        [0.3137, 0.0365, 0.6498]])\n",
      "Summing individual batch elements:\n",
      "tensor([1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# data = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "data = torch.randn(3, 3)\n",
    "print(f\"Original data:\\n{data}\")\n",
    "data = F.softmax(data, dim=1)\n",
    "print(f\"Transformed data:\\n{data}\")\n",
    "print(f\"Summing individual batch elements:\\n{data.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klassifisering av sifre\n",
    "Vi skal sette opp et nevralt nettverk som er i stand til å klassifisere sifre. Til det trenger vi det lett tilgjengelige MNIST-datasettet. Dette lastes ned gjennom torchvision-biblioteket. Torchvision-biblioteket er et hjelpebibliotek som tilbyr verktøy for Computer Vision-oppgaver. \n",
    "\n",
    "Datasett kommer vanligvis i et rå-format, og må behandles for å tilpasse det oppgaven vi skal gjøre. I dette tilfellet får vi PNG-bilder som må gjøres om til tensorer. Vi normaliserer også bildene for å få [bedre resultater](https://developers.google.com/machine-learning/data-prep/transform/normalization). Behandlingen gjøres med `torchvision.transforms`-biblioteket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Pipeline of processing operations\n",
    "image_processing = transforms.Compose([\n",
    "    transforms.ToTensor(), # Cast into torch.Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Pixel-values will range in [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root='./data', train=True, transform=image_processing, download=True)\n",
    "test_dataset = MNIST(root='./data', train=False, transform=image_processing, download=True) # Test data for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I maskinlæring trener man flere epoker (iterasjoner) på samme datasettet. \n",
    "\n",
    "Hver epoke inneholder igjen flere iterasjoner som består av å oppdatere vektene på et lite subset av datasettet. Man kaller dette for en batch. Denne logikken oppnår vi delvis gjennom `torch.utils.data.DataLoader`. Resten kommer når selve treningen skjer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data batch: torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32) # Iterable that provides 16 data samples each iteration\n",
    "\n",
    "data, labels = next(iter(train_loader)) # Retrieve a batch of data samples and labels for inspection purposes\n",
    "print(f\"Shape of data batch: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensoren inneholder 32 eksemplarer, 1 fargekanal (grayscale), 28 piksler i høyden, og 28 piksler i bredden.\n",
    "Vi kan visualisere et tilfeldig eksemplar fra hele datasettet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shown below is the digit 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKgAAACoCAAAAABRIPpoAAACMElEQVR4nO3bv8tOYRgH8I83vclo8GOWiYmS/8CgFAtGZLAog81kUSZFLIrFj0QyvEoW2VnkP0BKmcxew92po4fhfe/rUXd9P+N56jrfrrrOfc65n0NEREREREREjGpLVaED4DXYDVbALXCpu/5Kd4X/JEGrJWi1rf0l2ryvgV1gHfwCB/tPgIE6mqDVErRawdSfBdvAM3AMbO8vPTNMRxO0WoJWK5j6d+A52AFOzH69138CDNTRBK2WoNUKpv4lWAW3Z0UfgQf9J8BAHU3QaglarWDq27zfBOcwPdev9ZeeGaajCVotQasVvMO/Aq7PjnwAh/tLzwzT0QStlqDVCtb6rwtH2nv7u+Bi/wkwUEcTtFqCVivbr78AroI9mC4o7ZpwFHzqqD9MRxO0WoJWK5v6uRfgOKZn/Lajd7qj5jAdTdBqCVptKVPfPASnwA+wH3zfVLVhOpqg1RK0WsFz/b+8wjT1bR9/taPaMB1N0GoJWm2JU/8edTcTw3Q0QaslaLUlTn17v7deVG2YjiZotQSttsSp37dw5BD4sqlqw3Q0QaslaLWlTH3bqW/f4LS1/gl421FzmI4maLUErbbE7+tb6Y+Y9uu/ddQfpqMJWi1Bq21grT+Jv/0T7zI4gj+/r38DzqNv3pthOpqg1RK02gamfid4unC8relt0j+DO+BGT64Fw3Q0QaslaLUNTP1jsBecwXQn/xNcA/cx/Sen1jAdTdBqCRoRERERERERMfkNFRI0OyzjvJUAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=168x168>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a function to easily visualize MNIST instances\n",
    "def visualize(tensor):\n",
    "    h, w = tensor.shape[-2:]\n",
    "    image = transforms.functional.resize((tensor+1)/2, (h*6, w*6), interpolation=transforms.InterpolationMode.NEAREST) # Upscaling for visual purposes\n",
    "    image = transforms.functional.to_pil_image(image)\n",
    "    return image\n",
    "\n",
    "import random\n",
    "rand_index = random.randint(0, len(train_dataset)-1)\n",
    "data_sample, label_sample = train_dataset[rand_index]\n",
    "\n",
    "print(f\"Shown below is the digit {label_sample}\")\n",
    "visualize(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og tensoren i seg selv ser slik ut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8196, -0.4275,\n",
       "           0.5216,  0.1843,  0.1843, -0.8275, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.3098,  0.4353, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.0510,  0.9922,\n",
       "           0.9922,  0.9922,  0.7098, -0.1216, -1.0000, -1.0000, -1.0000,\n",
       "          -0.2627,  0.8510, -0.2000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.0510,  0.9922,\n",
       "           0.3882, -0.9216, -0.9529, -1.0000, -1.0000, -0.9608, -0.4588,\n",
       "           0.9059,  0.0275, -0.9843, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.0510,  0.9922,\n",
       "          -0.0745, -1.0000, -1.0000, -1.0000, -1.0000,  0.1137,  0.9922,\n",
       "           0.6078, -0.8275, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1373,  0.9922,\n",
       "           0.8902, -1.0000, -1.0000, -0.9608,  0.1059,  0.9843,  0.6000,\n",
       "          -0.8353, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9059,  0.7961,\n",
       "           0.9686, -0.7725, -0.7725,  0.1373,  0.9922,  0.5922, -0.8275,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.0745,\n",
       "           0.9922,  0.6784,  0.6863,  0.9922,  0.6157, -0.8353, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5059,\n",
       "           0.9922,  0.9922,  0.9922,  0.5765, -0.8275, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4902,\n",
       "           0.9922,  0.9922,  0.9922, -0.8353, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.2471,  0.7333,\n",
       "           0.9922,  0.9922,  0.9922, -0.9608, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -0.2157,  0.9451,  0.9922,\n",
       "           0.3569,  0.9922,  0.9922, -0.3098, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -0.6078,  0.7412,  0.9765, -0.0667,\n",
       "          -0.9529,  0.9922,  0.9922, -0.1451, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000,  0.8431,  0.9922, -0.0275, -1.0000,\n",
       "          -0.9843,  0.4118,  0.9922, -0.1451, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.9608,  0.1137,  0.9843,  0.0196, -0.9843, -1.0000,\n",
       "          -1.0000, -0.2000,  0.9922, -0.1451, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.7098,  0.9922,  0.6000, -0.8275, -1.0000, -1.0000,\n",
       "          -1.0000, -0.5843,  0.9922, -0.1451, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000,  0.0902,  0.9922, -0.4275, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.1843,  0.9922, -0.1451, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.4980,  0.9686,  0.9922, -0.8431, -1.0000, -1.0000, -1.0000,\n",
       "          -0.7490,  0.7490,  0.9922, -0.1451, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.2863,  0.9922,  0.9922, -0.7647, -0.9137, -0.8667, -0.0980,\n",
       "           0.7098,  0.9922,  0.6078, -0.8667, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000,  0.7412,  0.9922,  0.9922,  0.9922,  0.9922,  0.9922,\n",
       "           0.9843,  0.3804, -0.8510, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.6784,  0.1765,  0.9294,  1.0000,  0.9922,  0.4588,\n",
       "          -0.5137, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vi trenger et nettverk\n",
    "Alle lag i PyTorch arver fra `nn.Module`. Fra [dokumentasjonen](https://pytorch.org/docs/stable/generated/torch.nn.Module.html):\n",
    "\n",
    ">Base class for all neural network modules. \n",
    ">\n",
    ">Your models should also subclass this class.\n",
    ">\n",
    ">Modules can also contain other Modules, allowing to nest them in a tree structure.\n",
    "\n",
    "Trestrukturen som snakkes om er veldig nyttig. Endringer vi gjør på toppen av treet vil propageres ned til enkeltmodulene. Feks det å flytte parametrene over på en annen device.\n",
    "\n",
    "Vi husker det enkle lineære laget og tanh-funksjonen. De arver nemlig fra `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.module.Module'>\n",
      "<class 'torch.nn.modules.module.Module'>\n"
     ]
    }
   ],
   "source": [
    "print(layer.__class__.__base__)\n",
    "print(tanh.__class__.__base__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (layer2): Linear(in_features=200, out_features=42, bias=True)\n",
       "  (layer3): Linear(in_features=42, out_features=10, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=1*28*28, out_features=200) # Input features are the number of pixels, output features is arbitrary\n",
    "        self.layer2 = nn.Linear(in_features=200, out_features=42) # Arbitrary values\n",
    "        self.layer3 = nn.Linear(in_features=42, out_features=10) # 10 digits to differentiate between\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1) # Softmax will be computed for each batch element separately \n",
    "\n",
    "    def logits(self, data):\n",
    "        flattened_data = torch.flatten(data, start_dim=1, end_dim=-1) # Flatten the tensor from shape (batch_size, 1, 28, 28) to shape (batch_size, 1 * 28 * 28)\n",
    "\n",
    "        out = self.layer1(flattened_data)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        out = self.layer2(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        out = self.layer3(out)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, data):\n",
    "        logits = self.logits(data)\n",
    "        return self.softmax(logits)\n",
    "    \n",
    "model = Model() # Initialize model\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modellen vår implementerer to metoder. `logits` gir unnormaliserte verdier og vil brukes under trening. `forward` bruker `softmax` til å normalisere output fra `logits`, og tas i bruk under _inference_.\n",
    "\n",
    "Vi tester modellen på sifferet vi visualiserte tidligere. Men først må vi endre litt på _shapen_ til dette sifferet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data_sample.shape)\n",
    "test_input = data_sample[None, ...] # [None, ...] adds a new shape dimension in the front\n",
    "print(test_input.shape)\n",
    "test_input = test_input.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data_sample[None, ...]` lager en kopi av `data_sample` med en ny dimensjon lagt til.\n",
    "De fleste modulene i PyTorch forventer at input skal ha en batch-dimensjon, selv om batchen inneholder bare en instans (et bilde i dette tilfellet).\n",
    "\n",
    "Kodesnutten er veldig kryptisk. \n",
    "- PyTorch tolker indeksering med typen `None` som at en ny shape-dimensjon skal lages. \n",
    "- De etterfølgende `...` samler de resterende shape-dimensjonene. Dette er det samme som `data_sample[None, :, :, :]`.\n",
    "- `test_input` består av de nøyaktige samme elementene, men _organisert_ på en annen måte. \n",
    "\n",
    "En tilsvarende måte å skrive det på er `data_sample.unsqueeze(dim=X)`. Denne setter inn en ny (tom) dimensjon ved den spesifiserte dimensjonen. På samme måte kan man fjerne (tomme) dimensjoner med `data_sample.squeeze(dim=X)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1183, 0.0912, 0.0665, 0.0896, 0.1101, 0.0980, 0.1076, 0.0853, 0.1152,\n",
      "         0.1181]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "The untrained model predicts the digit to be 0\n"
     ]
    }
   ],
   "source": [
    "out = model.forward(test_input)\n",
    "print(out)\n",
    "print(f\"The untrained model predicts the digit to be {out.argmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening\n",
    "Vi ser fra resultatene at modellen ikke er i stand til å avgjøre hvilket siffer inputten var. \n",
    "\n",
    "Vi har modellen og datasettet. Da er det to viktige ting som mangler for å kunne oppnå en fungerende modell. \n",
    "- **En loss-funksjon som definerer objectivet**\n",
    "    - Vi ønsker at modellen skal gi høyest sannsynlighet på det rette sifret.\n",
    "    \n",
    "- **En algoritme som utfører gradient descent, altså selve maskinlæringen**\n",
    "    - Denne algoritmen skal ta utgangspunkt i losset for å optimere parametrene.\n",
    "    - Man implementerer slike aldri selv, og finner dem heller gjennom `torch.optim`.\n",
    "\n",
    "#### Loss-funksjoner\n",
    "Loss-funksjoner sammenlikner en prediction og et target. Prediction er output fra modellen, og target er fasiten vi vet fra datasettet. \n",
    "Den enkleste er _Mean Squared Error (MSE)_, som kalkulerer gjennomsnittlig kvadrert avvik mellom tilsvarende elementer i hver tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(0.1775)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(size=(3, 4, 5))\n",
    "b = torch.zeros(size=(3, 4, 5))\n",
    "loss = F.mse_loss(a, b)\n",
    "print(loss)\n",
    "\n",
    "a = torch.rand(size=(3, 4, 5))\n",
    "b = torch.rand(size=(3, 4, 5))\n",
    "loss = F.mse_loss(a, b)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "Helt fram til beregningen av et loss konstrueres det en såkalt _computational graph_. Denne kan propageres bakover for å beregne gradienter (derav det kjente navnet [backpropagation](https://simple.wikipedia.org/wiki/Backpropagation) som brukes i alle nevrale nettverk idag).\n",
    "- Fra matematikken vet vi at gradienter peker i retning hvor verdien øker mest i en flervariabel kurve. Beveger vi oss i motsatt retning vil verdien minske.\n",
    "- Learning rate (en skalar verdi) styrer hvor store stegene er. \n",
    "\n",
    "Figuren under viser loss-funksjonen kalkulert for forskjellige verdier av parametrene $\\theta_1$ og $\\theta_2$ til en modell. \n",
    "\n",
    "[<img src=\"https://zitaoshen.rbind.io/project/optimization/1-min-of-machine-learning-gradient-decent/featured_hubf6ae7b9a0510d717632b017746fdfc1_374655_720x0_resize_lanczos_2.png\">](https://zitaoshen.rbind.io/project/optimization/1-min-of-machine-learning-gradient-decent/)\n",
    "\n",
    "<!-- Oppdatering av vektene gjøres da slikt:\n",
    "\n",
    "$\\boldsymbol{w} \\leftarrow \\underbrace{\\alpha}_{\\text{learning rate}}$ -->\n",
    "\n",
    "Selv om konkrete regler finnes for beregning av gradientene til vektene, er det svært kronglete å gjøre manuelt. Derfor takker vi AI-gudene for bibliotek som PyTorch og Tensorflow som gjør denne prosessen så og si automatisk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tilbake til treningen av en siffer-gjenkjenneren\n",
    "Til loss-funksjonen bruker vi `nn.CrossEntropyLoss`. Den sammenlikner to sannsynlighetsfordelinger. Matematikken bak er ikke så viktig.\n",
    "\n",
    "Til gradient descent bruker vi bruker Adam, som står for _Adaptive Moment Estimation_. Her er heller ikke matematikken viktig. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1875/1875 [00:07<00:00, 247.21batch/s]\n",
      "Epoch 1: 100%|██████████| 1875/1875 [00:07<00:00, 248.97batch/s]\n",
      "Epoch 2: 100%|██████████| 1875/1875 [00:07<00:00, 244.98batch/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm # Progress bar\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm(train_loader, unit=\"batch\") as pbar:\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        for i, (data, labels) in enumerate(pbar):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model.logits(data) # Query model for predictions\n",
    "            loss = loss_fn(pred, labels)\n",
    "            \n",
    "            loss.backward() # Propagate the computational graph and calculate gradients\n",
    "            optimizer.step() # Uses the calculated gradients on the registered parameters to perform an update\n",
    "            optimizer.zero_grad() # Remove the gradients\n",
    "\n",
    "            # pbar.set_postfix(loss=loss.cpu().item()) if i%40 == 0 else None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing av modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model on the test set is 0.09039999544620514\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "correct = 0\n",
    "for data, labels in test_loader:\n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    pred = model.forward(data)\n",
    "    correct += torch.sum(pred.argmax(dim=1) == labels)\n",
    "accuracy = correct/len(test_dataset)\n",
    "\n",
    "print(f\"The accuracy of the model on the test set is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.7730e-04, 1.2660e-04, 7.4894e-03, 4.8770e-06, 9.7188e-01, 2.4424e-04,\n",
      "         1.5556e-02, 2.5305e-03, 6.4534e-04, 1.0469e-03]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "The trained model predicts the digit to be 4\n"
     ]
    }
   ],
   "source": [
    "out = model.forward(test_input)\n",
    "print(out)\n",
    "print(f\"The trained model predicts the digit to be {out.argmax()}\")\n",
    "# plt.bar(torch.linspace(0, 9, 10).numpy(), out[0].cpu().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder\n",
    "\n",
    "For å komme litt inn i tankegangen om vektorrepresentasjoner av tekst (embeddings), ser vi på en arkitektur som kalles for AutoEncoder. \n",
    "\n",
    "<img src=\"autoencoder.png\" height=\"300px\">\n",
    "\n",
    "Den består av to komponenter:\n",
    "- Encoder\n",
    "    - En sekvens med lineære lag som reduserer dimensjonaliteten.\n",
    "    - Ender opp med en vektor med veldig få dimensjoner kontra 784 fra rådata.\n",
    "- Decoder\n",
    "    - En sekvens med lineære lag som rekonstruerer inputten fra den korte vektoren.\n",
    "\n",
    "Vi tar i bruk `nn.Sequential` for å lage sekvenser av lag som dataen flyter gjennom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=200, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=200, out_features=42, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=42, out_features=16, bias=True)\n",
       "    (5): Tanh()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=42, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=42, out_features=200, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=200, out_features=784, bias=True)\n",
       "    (5): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=200, out_features=42),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=42, out_features=16),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "             nn.Linear(in_features=16, out_features=42),\n",
    "             nn.Tanh(),\n",
    "             nn.Linear(in_features=42, out_features=200),\n",
    "             nn.Tanh(),\n",
    "             nn.Linear(200, 28*28),\n",
    "             nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, data):\n",
    "        out = torch.flatten(data, start_dim=1, end_dim=-1)\n",
    "        out = self.encoder(out)\n",
    "        return out\n",
    "    \n",
    "    def decode(self, data):\n",
    "        out = self.decoder(data)\n",
    "        return out.reshape(-1, 1, 28, 28)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.encode(data)\n",
    "        out = self.decode(out)\n",
    "        return out\n",
    "    \n",
    "autoencoder = AutoEncoder()\n",
    "autoencoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1875/1875 [00:08<00:00, 220.84batch/s, loss=0.118]\n",
      "Epoch 1: 100%|██████████| 1875/1875 [00:08<00:00, 225.15batch/s, loss=0.0832]\n",
      "Epoch 2: 100%|██████████| 1875/1875 [00:08<00:00, 217.87batch/s, loss=0.0782]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm(train_loader, unit=\"batch\") as pbar:\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        for i, (data, _) in enumerate(pbar): # no need for labels\n",
    "            data = data.to(device)\n",
    "            pred = autoencoder.forward(data) # Query model for predictions\n",
    "            loss = loss_fn(pred, data)\n",
    "            \n",
    "            loss.backward() # Propagate the computational graph and calculate gradients\n",
    "            optimizer.step() # Uses the calculated gradients on the registered parameters to perform an update\n",
    "            optimizer.zero_grad() # Remove the gradients\n",
    "\n",
    "            pbar.set_postfix(loss=loss.cpu().item()) if i%40 == 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi studerer nå hvordan modellen klarer å rekonstruere et siffer fra datasettet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKgAAACoCAAAAABRIPpoAAACqElEQVR4nO3bPchcRRSH8V/E4FeIFpLKQoQEksqAH402FpFUoiAIaUIIpBJSiI0iFhYiCFqopUVKIRELIYWKYGGhNoKkShULDSEpFBFeeS2GCxNXw87uiTDwf4plOcw983DgMPfOnUsIIYQQQgghhBBCCGEO9ty+1I+AD8Gz4CvwzEbZ7igw+l+IaDURrea2dP1R8BF4vIvvgOPgy8Gc01Q0otVEtJrirt8HvgcHwe7KZN9gWfF31s48TUUjWk1Eq7mzKtFd4C0s/d74E9zdRZ4C94Nra+efpqIRrSai1ZR1/avgZSzr+3XwOvgAy1q/axOmqWhEq4loNQVdfxq8thL/Ary9/QSYqKIRrSai1Wz1XP8QuATuxbKOnwXvd5F+sp/BYfDb2nNNU9GIVhPRajZc6x8GF8E9Xfwd8Ogtrz0A9g7OOE1FI1pNRKvZsOtP4eZdu8Y58G0XWX2Wb1OOVmiaika0mohWM9z1+8EZLL3cdunbzvwb4L5u/GZ7d6tMU9GIVhPRaoa7/l3wYBe5DH4Az6+RoZ3PuTE47zQVjWg1Ea1moOvb0ENY7tuvghPdmD3dbx9pK/4NcBL8NSDJRBWNaDURrWag6x/Aciffuridq/8RPIabe7z/387gvYJlD3+UaSoa0WoiWs3Am7t2V/9LF7kCfgLHbnntx1je7G/GNBWNaDURrWa4638dnOA9LGd4/hi8tmeaika0mohWM9D17Q371+DJNcZ/Dl4Cvw9J/RvTVDSi1US0muGzeUfAm1i+oGlf1jzdjWnP8i+C85ua/YNpKhrRaiJaTcGXthfAc13kU/DC9qk7pqloRKuJaDVbdX3r609WEj0Bvtsm9QrTVDSi1US0mq2+uevP5LS7+s+w7OrXMk1FI1pNREMIIYQQQgjhv/kb7EJHFPCsYcgAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=168x168>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_index = random.randint(0, len(train_dataset)-1)\n",
    "data_sample, label_sample = train_dataset[rand_index]\n",
    "\n",
    "visualize(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n",
      "tensor([[ 0.4411,  0.0683,  0.3689,  0.2224, -0.5183,  0.2511, -0.1764,  0.4652,\n",
      "         -0.2179,  0.6045, -0.4412, -0.2956, -0.2604, -0.4322, -0.6259, -0.6960]],\n",
      "       device='cuda:0', grad_fn=<TanhBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGAAAAAGCAAAAAAZXsSjAAAAR0lEQVR4nO3NQQ2AIAAAwJONAoawDxX8mYE2/khAB3+mcGPj7dcUjg9X4JYKbrCBDk6wgwskUMABMlhBBA084AXBz2YwPvgANesJMnBV2LMAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=96x6>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = autoencoder.encode(data_sample[None, ...].to(device)) # Add batch dimension\n",
    "encoding\n",
    "print(encoding.shape)\n",
    "print(encoding)\n",
    "visualize(encoding[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7786,  0.0768, -0.6494, -0.0700,  0.5428,  0.5079,  0.2910, -0.0614,\n",
       "          0.4854, -0.6032,  0.6029,  0.3430, -0.0687, -0.1591,  0.4080, -0.2909]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKgAAACoCAAAAABRIPpoAAADzklEQVR4nO3cy4olRRDG8d84tjJe8QbqeHdERUREBLeCuvA1fBOfy4240I0w7lRkUPE+6rQXvIx2j4swIYbq011nOhpJif+iaLLyZH8VEBmRkVlF0zRN0zT/Z05VDXHl+AMdynUnPH4ZLbSaFto0TXM8jhXr84871v9LC62mhVazhddH19PghtTyF9hL1/XcDP5Y8dtpLNpCq2mh1Vy/plM8TXj6LeAs+Al8j+G5S2KWuBe8Bp4Bv4J3wTtphCvputQwAS20mhZazRFefyp1ugM8Du4HX4Dd1DNX9nJucBM4B55KY36W7l4G++m3+0nJNBZtodW00GpWef0OuBu8lO5+heGbOTrnZcNeankIPIyRJ+xiWCv7eP5b6jMBLbSaFlrNRq/PkTpW38+Dl8HH4GeMGB3evcn3/waPYsweF8En4LfUM+cJebRpLNpCq2mh1Rzg9dlbI8rHKv5V8Aj4MfVZVuGWJcKI71ETiFniA/A1+BObV/TBNBZtodW00GqO8PrbwXPgMYx5YBd8g6tz+KXnRrbwdPptRPYPMWp6yzxhyTQWbaHVtNBqDvD67H2RmUekPptaPsKI2jupPft++Hvk8y+A2zDi+3ms8/dgGou20GpaaDVHrOvvAk9i7NzFkz0BHgTfYfj4pdQnav6vgxcxavWx0xeVwPVne6axaAutpoVWszHW34jh7xHlYx6IeP0KeACjphd347nPYMTxZ9MIkRVcTP1jpb+7Qug0Fm2h1bTQajbG+vDQiO+n0zVaYr/+Voz5Idojz4/qXOy+RWXgTLoba/k8S6xhGou20GpaaDUbq3nhs1+C98APGHW5yNJ/x5gN7gO/4OqaXmT1MTNE5f88RnzfdKJvyTQWbaHVtNBqDjiHnyty96RrRPaddDeqeTl2R0vE+jjP8yZGbvA2eANj3uh1/X9HC61mGqFH1PBjDR4xOvw9v2Wz6UxO9PwUw7ujsvcWRj6wLdNYtIVW00KrWfX2zZr36fIMsJ+usa6PPfpLqX1bprFoC62mhVazyuu3JZ4+9vUi24/VQcT9a3ulfxqLttBqWmg1xV6fK/Oxpx97c3uLPtsyjUVbaDUttJoTifUR2e/EyOcj4n+LUfPf9mt701i0hVbTQqs5kVif9+xib25ZwVu+j38401i0hVbTQqs5Ea8Pf493c94HFzCqebG7d/gbdkumsWgLraaFVlPwZexMPpF7Lv2DzzF29i8fY+QJaKHVtNBqimN9xO44eRtv5cSpvDirv+23NDPTWLSFVtNCqymO9ctBl1/AuzamsWgLraaFVvMPg1udC8M2r64AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=168x168>"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoding = autoencoder.decode(encoding)\n",
    "visualize(decoding.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ved å ha en flaskehals i midten av nettverket, har modellen lært seg en lav-dimensjonell representasjon av sifrene.\n",
    "\n",
    "Det morsomme er at decoder-delen av nettverket kan anvendes som en generativ modell. Inputtet vil være en vektor av lav dimensjon. Vi har også brukt `Tanh` som aktiveringsfunksjon på output fra encoderen, som betyr at decoderen alltid vil forvente tall som ligger i $\\langle-1, 1\\rangle$. \n",
    "\n",
    "Hvis vi generer en 16-dimensjonal vektor med tilfeldige tall, og bruker dette som input til decoderen, får vi et _syntetisk_ siffer som output. Den 16-dimensjonale vektoren vil i faglitteraturen kalles for en _latent_ representasjon av et siffer.\n",
    "\n",
    "Jeg skrev [masteroppgaven min](https://willdalh.github.io/thesis/thesis.pdf) om latente representasjoner i diffusjonsmodeller. Bare ta en titt 😄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKgAAACoCAAAAABRIPpoAAAFeUlEQVR4nO2dy4tcVRDGfz1zo446Jr7ABz6iICoYoigi6E4JuhTcudCtwZXLbALu/DPcuHAtqBCCgtmJC8HXQqNR1Gh8RZ1JnJnO4uvCrznTTUerIQfqW1zunHvuud9c+E7Vqapze7QCwJUAbAIwALDFv1gFYMeOIwB07zYAY+uJtV8BwHnrs2IjeE+/umOjbVt7Byii2Sii2RiJqY6X2YV/gFCc1CdVDnZ1ZO3Y1a2mJ9Z/pWnZac59nhmauy5pFNFsFNFsDP6HVCad6j9obffYjjvNcD4buJavBuCc3asZ5k4APrEn+vh7Gj4doIhmo4hmYyRdu9V2Lbf2Ou3BQGhf88kaEDOD4PNAN2+0iGajiGZjJHvartZ13Fzy4/09SfXrAPwGxEqhXRdc0iii2Sii2RikKbf44n6x9t3vHTfHWXBv/y4AHgfgewDesp7dvNEimo0imo1B3rWvoDUDbO3efxfof70cgAMAbADwDQB/ALtFAASp/loAngfgaQDeAeB9IOx+N2+0iGajiGZjEFNl1vx8EchG3wrAswAcBmJt/jYAbwDwIRDrd4eeeCMATwLh4Svf9zAAx6xnByii2Sii2Zh4+FLZ3wvfJr0/A4TSHwFCsxpzPxAeu+z4F8C0z6/3dBqAb4GIJMhb+N2e1c0bLaLZKKLZGOR7L+7Pa354FICXAHgAgKuA6dobxf9vAGDvjNFk/X8F4DsATgFwPRDxvcrcLQdFNBvdEJ3k67Wun+/ba70v/b4AwN1A5OLdgnsGUMeVpo/3lK6PAvAQAO/aE1dthA5QRLNRRLMx8fDn690rbV4E4CAQM4DstVYHUqh8culddvzU3PHH1vMEEBXCGtMziR2giGajiGZjmFUb39pled1av98ORNxeq/KzANwExCpAoymG/9MCVGTxNVfssRHcW+gARTQbRTQbg5Qre63zc00n/TfKyilG95e1a0V/HTAdzZNa7wVixvgBmJ5P2kr+sfHxavxu3mgRzUYRzcagmLnvdGutvNQnpX8GwGNAzA/y8+XVu7egMfcBcAcQPrxsuqz5mrVjHDT/bNqY3bzRIpqNIpqNSTSv3VvndTW6+gEQtTfPAfAEEDtobgFCrb5nR5p9CghbL59fK3f5CVodeB5B7ZpVND9080aLaDaKaDZG0mm7i7atyHWbK8su5crWK57/MgCHrKcUfQSA9wD4BYh6vJsBOG4chH1AzA8VzVsOimg2uiE6uIWdn7v3FfeGHc8AcBIItR6y0b4EwkPw9b5avrKRsT6K3l8DhJ/fzRstotkootkYPGI2qwK/3VPTQve+Yv2lVtXTfg5EJP+sPbGFZ/A1q9S3dJaDIpqNbogOvuutrZOnuTrL838QiDi/5+xUhy+LP0vpLcRK809V6SwHRTQb3RCdRPMUN/McmZ9r/e7fvtMMIHUrPv+a3aVo/6sAfGztF4vK1y8TRTQb3RCdsvXSflurIxWr9ka5Nq+lUR2+8vWK1Gmf7OvAbP9hEfjKops3WkSzUUSzMfW1TFeZr+69Gmfdzu8B4H4g5gGp/k0g1u//kZY9t76WuRwU0Wx0Q3Ti4XukTk3KqZ229m27ehsQWfiPgIjhfwrAn/+DkOtdWUXf+98Bimg2img2hjY+Lz//RyDq7aU75eWVTbsPgK+BqKz7GYjv3sz/apbQ7sKT3r0mp/2KfgcootkootkY2iZxl2U/CYTupOuDQETs1+0uRem19vd9NC1WZ7R4BMB3A1VF7nJQRLPRDdGh/e0JwbPkvivnjLWoxkYVd+ft6PFArGVs5/5VXq8HVjyh/SZ/N2+0iGajiGZj1GbqW1WuWR//Bau91kdxvI0FHjnY+F4jJPjvX7QRxQ5QRLNRRLNxAWqeHwRUgeY4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=168x168>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent = (torch.rand(size=(1, 16))-0.5)*2\n",
    "decoding = autoencoder.decode(latent.to(device))\n",
    "visualize(decoding.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultatene gjenspeiler den enkle arkitekturen som er valgt. Det er flere forbedringer man kan gjøre på arkitekturen for å få mer lovende sifre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppvarming til NLP\n",
    "Bilder har en naturlig representasjon som vektorer/tensorer. Piksler som ligger ved siden av hverandre har relativt like verdier og man får fine overganger mellom. Tekst er strukturert på en litt mer rotete måte, og virker mer abstrakt.\n",
    "\n",
    "Nå til dags distribueres modeller og kode gjennom en plattform som heter _Hugging Face_. Vi sørger først for at `transformers`-biblioteket som de drifter er lastet ned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers is already installed.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "library_name = 'transformers'\n",
    "try:\n",
    "    importlib.import_module(library_name)\n",
    "    print(f\"{library_name} is already installed.\")\n",
    "except ImportError:\n",
    "    print(f\"{library_name} is not installed. Installing...\")\n",
    "    %pip install {library_name}\n",
    "    print(f\"{library_name} has been installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De fleste NLP-pipeliner begynner med en tokenizer, som oversetter deler av en setning til IDer. Ulike modeller ledsager ulike tokenizers, derfor må vi spesifisere modell-navnet `bert-base-uncased`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi lager IDer av uttrykket \"Team Kamel\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: tensor([[  101,  2136, 27829,  2884,   102]])\n",
      "Tokens from IDs: ['[CLS]', 'team', 'kam', '##el', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Team Kamel\", return_tensors=\"pt\")\n",
    "print(f\"IDs: {inputs['input_ids']}\")\n",
    "print(f\"Tokens from IDs: {tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5640948414802551,\n",
       "  'token': 2307,\n",
       "  'token_str': 'great',\n",
       "  'sequence': 'team kamel will achieve great things'},\n",
       " {'score': 0.26359036564826965,\n",
       "  'token': 2116,\n",
       "  'token_str': 'many',\n",
       "  'sequence': 'team kamel will achieve many things'},\n",
       " {'score': 0.0189247727394104,\n",
       "  'token': 2204,\n",
       "  'token_str': 'good',\n",
       "  'sequence': 'team kamel will achieve good things'},\n",
       " {'score': 0.013463899493217468,\n",
       "  'token': 2122,\n",
       "  'token_str': 'these',\n",
       "  'sequence': 'team kamel will achieve these things'},\n",
       " {'score': 0.011696144007146358,\n",
       "  'token': 2035,\n",
       "  'token_str': 'all',\n",
       "  'sequence': 'team kamel will achieve all things'}]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "text = \"Team Kamel will achieve [MASK] things\"\n",
    "unmasker(inputs=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veien videre\n",
    "Det er mange spennende implementasjoner på arkitekturer der ute, og alle vil virke overveldende. Mitt beste tips er å starte med å leke mye med manipulering av tensorer og små nettverksarkitekturer. Få en god feeling på hva som virker og hvorfor det virker. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
