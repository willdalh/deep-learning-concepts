{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "Språkteknologi (NLP på engelsk) fikk en boost etter at [Transformer-arkitekturen](https://arxiv.org/abs/1706.03762) ble introdusert i 2017. Denne arkitekturen danner grunnlaget for de aller fleste tjenestene som kombinerer AI og språk idag, der ChatGPT er et godt eksempel. \n",
    "\n",
    "Siden da har [Hugging Face](huggingface.co) dukket opp som plattform som forsøker å tilgjengeliggjøre datasett, implementasjoner og erfaringer innen NLP. Av den grunn vil denne notebooken ta i bruk bibliotekene som tilbys fra dem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fyll inn det manglende ordet\n",
    "Modellen som først demonstrerte styrken til Transformere, er BERT, som står for _Bidirectional Encoder Representations from Transformers_. Den lager altså vektorrepresentasjoner av ord ved å se på både ordene som kom før, samt de etterfølgende ordene i en setning. Nasjonalbiblioteket har trent en norsk versjon av denne, som vi finner på Hugging Face. \n",
    "\n",
    "Vi setter opp en unmasker som tar i bruk en BERT-modell i bunnen for å beregne hvilket ord som mangler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='NbAiLab/nb-bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'journal'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = unmasker(\"I Norge brukes elektronisk [MASK] på sykehusene\")\n",
    "res[0]['token_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sammenlikne setninger\n",
    "BERT lager bare embeddinger av enkeltord, men i noen tilfeller ønsker vi å ha embeddinger for hele setninger, slik at vi kan sammenlikne dem. Til dette bruker vi en Sentence Transformer, og igjen har Nasjonalbiblioteket trent en som vi finner på Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('NbAiLab/nb-sbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "anchor = \"Jeg liker å spise iskrem\"\n",
    "anchor_embedding = model.encode(anchor, convert_to_tensor=True)\n",
    "print(anchor_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ser at setningen er er blitt gjort om til en vektor med 768 tall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Is er veldig godt\", \"Norske sykehus bruker elektronisk pasientjournal\"]\n",
    "sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi skal nå sammenlikne embeddingene av de to setningene med anker-setningen. Embeddingene er vektorer/punkter, og det er flere måter å gjøre dette på. Vi bruker cosinus-likhet, som er cosinus av vinkelen som spennes mellom to vektorer. Men man kunne også brukt Euklidsk avstand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with the sentence 'Jeg liker å spise iskrem':\n",
      "Is er veldig godt: 0.669\n",
      "Norske sykehus bruker elektronisk pasientjournal: 0.069\n"
     ]
    }
   ],
   "source": [
    "similarities_with_anchor = []\n",
    "for emb in sentence_embeddings:\n",
    "    cos_sim = util.cos_sim(anchor_embedding, emb).item()\n",
    "    similarities_with_anchor.append(cos_sim)\n",
    "\n",
    "print(f\"Similarities with the sentence '{anchor}':\")\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"{sentence}: {similarities_with_anchor[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likhetssøk i database\n",
    "Vi ser at embeddinger fungerer bra for å sammenlikne konteksten mellom to setninger, uten at ordene som brukes trenger å være helt like. Av denne grunn har det dukket opp flere tjenester som tilbyr å lagre embeddinger og utføre kjappe likhetssøk mellom dokumenter, der eksempler er [Redis Vector Database](https://redis.io/docs/get-started/vector-database/) og [Pinecone](https://www.pinecone.io/).\n",
    "\n",
    "For å utforske hvordan vektordatabaser fungerer i praksis, lager vi en enkel implementasjon selv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDatabase:\n",
    "    def __init__(self, model: SentenceTransformer):\n",
    "        self.documents = [] # list of strings\n",
    "        self.embeddings = [] # list of torch.Tensors\n",
    "        self.model = model # The SentenceTransformer model to use\n",
    "\n",
    "    def add_document(self, document: str) -> None:\n",
    "        \"\"\"\n",
    "        Add a document to the database. The document will be embedded using the model.\n",
    "\n",
    "        Args:\n",
    "            document (str): The document to add\n",
    "        \"\"\"\n",
    "        self.documents.append(document)\n",
    "        emb = self.model.encode(document, convert_to_tensor=True)\n",
    "        self.embeddings.append(emb)\n",
    "\n",
    "    def similarity_search(self, anchor_document: str, top_k: int = 3):\n",
    "        \"\"\"\n",
    "        Search for the most similar documents to the anchor document.\n",
    "\n",
    "        Args:\n",
    "            anchor_document (str): The document to search for\n",
    "            top_k (int, optional): The number of documents to return. Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tuples containing the document and the similarity score\n",
    "        \"\"\"\n",
    "        anchor_emb = self.model.encode(anchor_document, convert_to_tensor=True) # Embed the anchor document\n",
    "        similarities = []\n",
    "        for emb in self.embeddings: # Iterate all the document embeddings\n",
    "            cos_sim = util.cos_sim(anchor_emb, emb).item()\n",
    "            similarities.append(cos_sim)\n",
    "        top_k_similar_indices = torch.topk(torch.tensor(similarities), k=top_k).indices.tolist() # Get the indices of the top k most similar documents\n",
    "        res = [(self.documents[i], similarities[i]) for i in top_k_similar_indices] # Get the documents and their similarity scores\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [ \n",
    "    \"På norske sykehus brukes det elektroniske pasientjournaler\", \n",
    "    \"Bier er viktige for pollinering av mange av våre matvekster.\",  \n",
    "    \"Mange nordmenn elsker å gå på ski om vinteren.\",  \n",
    "    \"Astronomi er studiet av universet og dets himmellegemer.\",\n",
    "    \"Ute i verdensrommet finnes det mange planeter og stjerner.\",\n",
    "    \"Sjokolade er en populær godbit som lages fra kakaobønner.\",  \n",
    "    \"Fotball er en av de mest populære sportene i verden.\",  \n",
    "    \"Paris er kjent for sin arkitektur, mat og mote.\",  \n",
    "    \"Elefanter er det største landdyret på jorden.\",  \n",
    "    \"Bøker er en viktig kilde til kunnskap og underholdning.\",  \n",
    "    \"Musikk er en universell form for kunst som uttrykker følelser og ideer.\",  \n",
    "    \"Klimaendringer er en stor utfordring for verden i dag.\",\n",
    "    \"Taco er vanlig å spise på fredager\"\n",
    "]  \n",
    "\n",
    "vector_db = VectorDatabase(model)\n",
    "for document in corpus:\n",
    "    vector_db.add_document(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Klimaendringer er en stor utfordring for verden i dag.',\n",
       "  0.5642093420028687)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_document = \"Global oppvarming ødelegger jordkloden\"\n",
    "similar_documents = vector_db.similarity_search(anchor_document, top_k=1)\n",
    "similar_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Astronomi er studiet av universet og dets himmellegemer.',\n",
       "  0.4471147060394287),\n",
       " ('Ute i verdensrommet finnes det mange planeter og stjerner.',\n",
       "  0.43058377504348755),\n",
       " ('Musikk er en universell form for kunst som uttrykker følelser og ideer.',\n",
       "  0.1297227144241333)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_sentence = \"Når det er mørkt og skyfritt kan man se stjerner\"\n",
    "similar_sentences = vector_db.similarity_search(anchor_sentence, top_k=3)\n",
    "similar_sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
