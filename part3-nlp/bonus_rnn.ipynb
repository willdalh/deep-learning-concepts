{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model trained to predict the next word in a sentence\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, target_sentences):\n",
    "        super().__init__()\n",
    "        self.target_sentences = [f\"<START> {sentence.strip()} <END>\" for sentence in target_sentences]\n",
    "        self.vocab = list(set(\" \".join(self.target_sentences).split(\" \")))\n",
    "\n",
    "        self.hidden_size = 128\n",
    "        self.embedding_size = 64\n",
    "\n",
    "        self.last_layer = nn.Linear(self.hidden_size, len(self.vocab))\n",
    "        self.embedding = nn.Embedding(len(self.vocab), self.embedding_size)\n",
    "        self.internal_layer = nn.Linear(self.embedding_size + self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def tokenize(self, word):\n",
    "        return torch.LongTensor([self.vocab.index(word)])\n",
    "\n",
    "    def logits(self, sentence):\n",
    "\n",
    "        # Initialize hidden state for sequence\n",
    "        hidden = torch.zeros(1, self.hidden_size)\n",
    "\n",
    "        # Process each token in the sequence\n",
    "        split = sentence.split(\" \")\n",
    "        # Check if the word is in the vocab\n",
    "        if not all([word in self.vocab for word in split]):\n",
    "            raise ValueError(\"Word not in vocab\")\n",
    "        \n",
    "        # Iterate through each word in the sequence\n",
    "        for elem in split:\n",
    "            token = self.tokenize(elem)\n",
    "            embedding = self.embedding(token)\n",
    "            input_data = torch.cat((embedding, hidden), dim=1)\n",
    "            hidden = self.internal_layer(input_data)\n",
    "            \n",
    "        # The last hidden state should go through the last layer\n",
    "        output = self.last_layer(hidden)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_prob_dist_over_next_word(self, sentence: str):\n",
    "        dist = self.forward(sentence)[0]\n",
    "        return {self.vocab[i]: f\"{dist[i].item():.2f}\" for i in range(len(dist))}\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        return F.softmax(self.logits(sentence), dim=1)\n",
    "    \n",
    "    def predict(self, sentence):\n",
    "        # sentence = f\"<START> {sentence}\"\n",
    "        result = self.vocab[torch.argmax(self.forward(sentence))]\n",
    "        # if result == \"<END>\":\n",
    "        #     return \"\"\n",
    "        return result\n",
    "    \n",
    "    def complete_sentence(self, sentence, max_len=100):\n",
    "        result = sentence.strip()\n",
    "        new_token = \"\"\n",
    "        while not new_token == \"<END>\":\n",
    "            new_token = self.predict(result)\n",
    "            if len(result.split(\" \")) >= max_len:\n",
    "                break\n",
    "            result += \" \" + new_token\n",
    "        return result\n",
    "\n",
    "    def train(self, num_epochs=10):\n",
    "        for epoch in tqdm(range(num_epochs), unit=\"epoch\"):\n",
    "            for sentence in self.target_sentences:\n",
    "                split_sentence = sentence.split(\" \")\n",
    "            \n",
    "                for i in range(len(split_sentence) - 1):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    input_data = split_sentence[:i+1]\n",
    "                    target = split_sentence[i+1]\n",
    "                    output = self.forward(\" \".join(input_data))\n",
    "                    loss = self.loss_fn(output, self.tokenize(target))\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Før vi trener modellen sjekker vi at den **ikke** klarer å skrive setninger som gir mening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi mye et er fordi maskinlæring fordi'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sentences = [\"dette er et kodekurs i maskinlæring\", \"kodekurset er gøy fordi man lærer mye\"]\n",
    "rnn = RNNModel(target_sentences)\n",
    "rnn.complete_sentence(\"<START> \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 22.13epoch/s]\n"
     ]
    }
   ],
   "source": [
    "rnn.train(num_epochs=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> dette er et kodekurs i maskinlæring <END>'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.complete_sentence(\"<START> dette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dette': '0.00',\n",
       " 'lærer': '0.00',\n",
       " 'kodekurs': '0.00',\n",
       " '<END>': '0.00',\n",
       " 'gøy': '1.00',\n",
       " 'er': '0.00',\n",
       " 'kodekurset': '0.00',\n",
       " 'mye': '0.00',\n",
       " 'fordi': '0.00',\n",
       " 'maskinlæring': '0.00',\n",
       " '<START>': '0.00',\n",
       " 'et': '0.00',\n",
       " 'i': '0.00',\n",
       " 'man': '0.00'}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.get_prob_dist_over_next_word(\"<START> kodekurset er\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
