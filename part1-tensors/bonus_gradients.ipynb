{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradienter\n",
    "Fra `tensor.ipynb` i del 1 så vi på slutten at tensorer ikke er som helt vanlige flerdimensjonale Python lister, siden vi kan spesifisere hvilken GPU de skal ligge på.  \n",
    "En annen viktig funksjonalitet som PyTorch tilbyr på denne datastrukturen er loggføring av hvilke operasjoner som utføres på tensorer, noe som er essensielt når nevrale nettverk trenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loggføringen skjer bare på tensorer hvor flagget `requires_grad` settes, og på tensorer som deltar i en operasjon med en annen tensor som har det aktivert.  \n",
    "Vi begynner med et enkelt eksempel for å vise hvordan denne loggen ser ut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4794, 0.5701],\n",
       "        [0.9579, 0.1469]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(2, 2, requires_grad=True)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4794, 2.5701],\n",
       "        [2.9579, 2.1470]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor + 2\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24.7945, 25.7011],\n",
       "        [29.5788, 21.4695]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor * 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4794, 2.5701],\n",
       "        [2.9579, 2.1470]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor / 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etter hver operasjon får den nye tensoren en `grad_fn` tilknyttet seg som beskriver historikken. Loggen som konstrueres kalles for en _computational graph_, og vi kan sjekke den for å se at operasjonene er dokumentert. \n",
    "\n",
    "**Merk**: Dette gjør vi for moro skyld. Det er ikke noe man gjør i praksis. Hvordan PyTorch tar i bruk loggen for å trene nevrale nettverk skjer automatisk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DivBackward0 object at 0x7f83001f3f70>\n",
      "<MulBackward0 object at 0x7f83001f33a0>\n",
      "<AddBackward0 object at 0x7f83001f3f70>\n"
     ]
    }
   ],
   "source": [
    "grad_fn = tensor.grad_fn\n",
    "while len(grad_fn.next_functions) != 0:\n",
    "    print(grad_fn)\n",
    "    grad_fn = grad_fn.next_functions[0][0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
