{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppsett av nevrale nettverk med PyTorch\n",
    "Nevrale nettverk har tilsynelatende komplekse strukturer. Likevel består de av flere enkle og isolerte komponenter. Disse finner man i `torch.nn`. \n",
    "\n",
    "De fleste komponentene blir et _lag_ i nettverket, men det finnes komponenter som anvendes på eksisterende lag (feks aktiveringsfunksjoner). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den enkleste komponenten man finner i et nevrale nettverk er lineære lag. Matematisk gjør den en lineær transformasjon fra et vektorrom til et annet. Feks kan det lineære laget ta inn en vektor med 5 features, og outputte en vektor med 3 features:\n",
    "\n",
    "<div >\n",
    "<img src=\"../res/nn_5in_3out-cropped.svg\" width=\"500px\" height=\"auto\" alt=\"SVG Image\"  style=\"filter: invert(100%);\"/>\n",
    "</div>\n",
    "\n",
    "I PyTorch lager vi lineære lag gjennom `torch.nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=3, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3) # Construct layer\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector: tensor([[1., 1., 1., 1., 1.]])\n",
      "Output vector: tensor([[-1.1653, -0.2081, -0.4378]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data = torch.Tensor([[1, 1, 1, 1, 1]])\n",
    "print(f\"Input vector: {data}\")\n",
    "output = layer(data) # Feed data into layer\n",
    "print(f\"Output vector: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hva er det egentlig som har skjedd her? Vektoren, som består av 5 enere, har blitt til transformert til en vektor med 3 tall. Vi studerer _vektene_ til det lineære laget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2099,  0.1273, -0.3920, -0.1973, -0.1920],\n",
      "        [-0.2092, -0.1029,  0.3795, -0.3177, -0.1137],\n",
      "        [-0.3444,  0.1065,  0.3293, -0.3918,  0.0028]], requires_grad=True)\n",
      "Weight shape: torch.Size([3, 5])\n",
      "\n",
      "Parameter containing:\n",
      "tensor([-0.3015,  0.1558, -0.1402], requires_grad=True)\n",
      "Bias shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(layer.weight)\n",
    "print(f\"Weight shape: {layer.weight.shape}\\n\")\n",
    "\n",
    "print(layer.bias)\n",
    "print(f\"Bias shape: {layer.bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når man sender input-vektoren inn skjer følgende operasjon (hvor `@` er matrisemultiplikasjon):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1653, -0.2081, -0.4378]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data @ layer.weight.T + layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ser fra resultatet over at det stemmer.\n",
    "\n",
    "\n",
    "Vektene og biaset utgjør til sammen **parametrene** for dette lineære laget, og er verdiene som endres når man trener nettverket. Hvordan disse er strukturert og brukes varierer for andre typer lag. Til denne introduksjon holder vi oss derimot til lineære lag, men sjekk ut `bonus_convnet.ipynb` for å se hvordan man bruker konvolusjonelle lag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktiveringsfunksjoner\n",
    "\n",
    "Det lineære laget utfører en lineær operasjon. Fleksibiliteten av nevrale nettverk kommer derimot av såkalte _aktiveringsfunksjoner_ som utfører ikke-lineære operasjoner på data. Disse inneholder **vanligvis ikke** trenbare parametre. De enkleste opererer på hvert element av en vektor individuelt, som feks Tanh. Vi ser på et en-dimensjonalt-case for å visualisere det enkelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt # Library for visualization\n",
    "tanh = nn.Tanh()\n",
    "assert not hasattr(tanh, \"weight\") # No learnable weights\n",
    "\n",
    "data = torch.linspace(start=-6, end=6, steps=100) # One-dimensional vector with 100 elements \n",
    "output = tanh(data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "\n",
    "ax.plot(data, output)\n",
    "ax.set_xlabel(\"Input data\")\n",
    "ax.set_ylabel(\"Tanh Output\")\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et viktig poeng er at aktiveringsfunksjoner ikke modifiserer _shapen_ til tensoren. Dette kan demonstreres på en tensor med flere shape-dimensjoner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before activation function: torch.Size([30, 5, 15])\n",
      "shape after activation function: torch.Size([30, 5, 15])\n",
      "But the content has changed: True\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(30, 5, 15)\n",
    "print(f\"shape before activation function: {data.shape}\")\n",
    "modified_data = tanh(data)\n",
    "print(f\"shape after activation function: {data.shape}\")\n",
    "print(f\"But the content has changed: {not torch.isclose(data, modified_data).all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En annen viktig er `nn.Softmax`, og brukes til å transformere en vektor til elementer som summeres til 1. \n",
    "\n",
    "Dette gjør den litt annerledes enn Tanh, siden vi må spesifisere en av shape-dimensjonene som skal summeres til 1.\n",
    "\n",
    "Den gjør seg svært godt som siste lag i et nettverk vi ønsker skal modellere en sannsynlighetsfordeling (Total sannsynlighet av alle utfallene av en stokastisk variabel skal være 1). Den er feks brukt som siste lag i ChatGPT, siden språkmodeller lærer seg en betinget sannsynlighetsfordeling over ord gitt tidligere tekst. \n",
    "\n",
    "For denne anledningen tar vi også i bruk `nn.functional`, som er et delbibliotek som tilbyr mange av komponentene tilstandsfrie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "tensor([[-0.9599,  0.9375],\n",
      "        [-1.0644,  0.5067],\n",
      "        [-1.7771,  0.9076]])\n",
      "Transformed data:\n",
      "tensor([[0.1304, 0.8696],\n",
      "        [0.1720, 0.8280],\n",
      "        [0.0639, 0.9361]])\n",
      "Summing individual batch elements:\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# data = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "data = torch.randn(3, 2)\n",
    "print(f\"Original data:\\n{data}\")\n",
    "data = F.softmax(data, dim=1)\n",
    "print(f\"Transformed data:\\n{data}\")\n",
    "print(f\"Summing individual batch elements:\\n{data.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klassifisering av sifre\n",
    "Vi skal sette opp et nevralt nettverk som er i stand til å klassifisere sifre. Til det trenger vi det lett tilgjengelige MNIST-datasettet. Dette lastes ned gjennom torchvision-biblioteket. Torchvision-biblioteket er et hjelpebibliotek som tilbyr verktøy for Computer Vision-oppgaver. \n",
    "\n",
    "Datasett kommer vanligvis i et rå-format, og må behandles for å tilpasse det oppgaven vi skal gjøre. I dette tilfellet får vi PNG-bilder som må gjøres om til tensorer. Vi normaliserer også bildene for å få [bedre resultater](https://developers.google.com/machine-learning/data-prep/transform/normalization). Behandlingen gjøres med `torchvision.transforms`-biblioteket. \n",
    "\n",
    "Datasettet lagres i roten av prosjektet i mappen `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Pipeline of processing operations\n",
    "image_processing = transforms.Compose([\n",
    "    transforms.ToTensor(), # Cast into torch.Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Pixel-values will range in [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root='../data', train=True, transform=image_processing, download=True)\n",
    "test_dataset = MNIST(root='../data', train=False, transform=image_processing, download=True) # Test data for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I maskinlæring trener man flere epoker (iterasjoner) på samme datasettet. \n",
    "\n",
    "Hver epoke inneholder igjen flere iterasjoner som består av å oppdatere vektene på et lite subset av datasettet. Man kaller dette for en batch. Denne logikken oppnår vi delvis gjennom `torch.utils.data.DataLoader`. Resten kommer når selve treningen skjer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data batch: torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32) # Iterable that provides 16 data samples each iteration\n",
    "\n",
    "data, labels = next(iter(train_loader)) # Retrieve a batch of data samples and labels for inspection purposes\n",
    "print(f\"Shape of data batch: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensoren inneholder 32 eksemplarer, 1 fargekanal (grayscale), 28 piksler i høyden, og 28 piksler i bredden.\n",
    "Vi kan visualisere et tilfeldig eksemplar fra hele datasettet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shown below is the digit 9\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKgAAACoCAAAAABRIPpoAAACtklEQVR4Ae2b203lQAxALyvKgG2DRx3AtoGgDqAOHnXwaIOlD5DsH0szScZjA7J0+BiMY3uc4+s7ySTsdvxAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAK/T2AvnsKVhDiW8VzGRxP0VuRXo5kT/8y5/bwXiWYzh2g20VDX30g22vXriWnv34nRx7rpwlFKvwBmWg3RaXQLjmWIurv+QM74QcaT5uyvRaN9fdnY/BfNhYze1b8MURKV+iYOEE2EKaHcXf8ubocmj/VefhbL9vvBOzGlN8hTRIimYDRByhDdN0kPibpG267/u+p3JUdfjI2VjXpDLEOURDcq6T4MUTeyDQd319t4I/2r9wLWS6/wrWZEpvQjlDw2EPXQGrEtQ9Td9Xo9P8LgXYzsVYF+S+hd/0gEa1OGKInasmXIEM2gaGN4b693uptne1/3598k6pmMatPey+s3AF1vC/B7Ms2Uzb4MUXfXK6mlPbp1jpOTSdAyREl0/UPgPwpRP7N1D/cVvoY7lV+6puta/2Q0uprfm5nteztG7RApvQPWkClEhzA5jMoQjSy/izx0v852vV7te5/R2wnKECVRW7YMGaIZFG2MybXehmjlo0YV6XcNRukbqEEFRIMAG/cyRL+l6/VtfIUy8nSvwddRlCFKop3qhVQQDeHrOCd3/bFM0T6t68zsVFF6J7BNc4huInIalCGa3PW6y2dh6X/cWM2cXIYoic4VeNkLosts5o4kd72u9TaVD/tHQKb0AXhdV4h2sQSUZYgmd70+y1NwWXf0Gq0MURIN9E3XFaJdLAFlctfb9/GyVnk9O0ofqHLXFaJdLAFlGaLJz+s/G2ZZE5QhSqLNZyCogGgQYOOevNbr+/m6h/+vmSyioPQRej1fiPaoRHRliEZOEl8IQKAigS9vxSyvSAOPaAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=168x168>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.visualize import visualize\n",
    "\n",
    "rand_index = torch.randint(0, len(train_dataset), (1,)).item()\n",
    "data_sample, label_sample = train_dataset[rand_index]\n",
    "print(f\"Shown below is the digit {label_sample}\")\n",
    "visualize(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og tensoren i seg selv ser slik ut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -0.1059,  0.3333,  1.0000,\n",
       "           0.3333,  0.3333, -0.5529, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000,  0.1059,  1.0000,  1.0000,  1.0000,\n",
       "           1.0000,  1.0000,  1.0000,  0.1059, -0.7725, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.7725,  0.5529,  1.0000,  1.0000,  1.0000, -0.1059,\n",
       "          -0.3255,  0.5529,  1.0000,  1.0000,  0.7725, -0.5529, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000,  0.7725,  1.0000,  1.0000,  0.7725, -0.5529, -1.0000,\n",
       "          -1.0000, -1.0000,  0.5529,  1.0000,  1.0000,  1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000,  0.3333,  1.0000,  1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -0.5529,  1.0000,  1.0000,  0.5529, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000,  0.3333,  1.0000,  0.5529, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000,  0.7725,  1.0000,  1.0000, -0.7725, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000,  0.1059,  1.0000,  0.5529, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000,  0.7725,  1.0000,  1.0000,  0.5529, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -0.7725,  1.0000,  1.0000,  0.1059, -0.3255,  0.3333,\n",
       "           0.5529,  1.0000,  1.0000,  1.0000, -0.7725, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000,  0.5529,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "           1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -0.7725,  0.3333, -0.3255, -0.1059,\n",
       "          -0.3255,  1.0000,  1.0000,  0.3333, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -0.3255,  1.0000,  1.0000, -0.5529, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           0.1059,  1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           0.7725,  1.0000,  0.5529, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5529,\n",
       "           1.0000,  1.0000,  0.1059, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1059,\n",
       "           1.0000,  1.0000, -0.7725, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3333,\n",
       "           1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5529,\n",
       "           1.0000,  0.5529, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
       "           1.0000,  0.3333, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
       "           1.0000,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7725,\n",
       "           1.0000, -0.3255, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vi trenger et nettverk\n",
    "Alle lag i PyTorch arver fra `nn.Module`. Fra [dokumentasjonen](https://pytorch.org/docs/stable/generated/torch.nn.Module.html):\n",
    "\n",
    ">Base class for all neural network modules. \n",
    ">\n",
    ">Your models should also subclass this class.\n",
    ">\n",
    ">Modules can also contain other Modules, allowing to nest them in a tree structure.\n",
    "\n",
    "Trestrukturen som nevnes er veldig nyttig. Endringer vi gjør på toppen av treet vil propageres ned til enkeltmodulene. Feks det å flytte parametrene over på en GPU.\n",
    "\n",
    "Vi husker det enkle lineære laget og tanh-funksjonen. De arver nemlig fra `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.module.Module'>\n",
      "<class 'torch.nn.modules.module.Module'>\n"
     ]
    }
   ],
   "source": [
    "print(layer.__class__.__base__)\n",
    "print(tanh.__class__.__base__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nettverket vi lager inneholder tre lag:\n",
    "1. `layer1` tar inn et $28 \\times 28$-bilde\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (layer2): Linear(in_features=200, out_features=42, bias=True)\n",
       "  (layer3): Linear(in_features=42, out_features=10, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=1*28*28, out_features=200) # Input features are the number of pixels, output features is arbitrary\n",
    "        self.layer2 = nn.Linear(in_features=200, out_features=42) # Arbitrary values\n",
    "        self.layer3 = nn.Linear(in_features=42, out_features=10) # 10 digits to differentiate between\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1) # Softmax will be computed for each batch element separately \n",
    "\n",
    "    def logits(self, data):\n",
    "        # Flatten the tensor from shape (batch_size, 1, 28, 28) to shape (batch_size, 1 * 28 * 28)\n",
    "        flattened_data = torch.flatten(data, start_dim=1, end_dim=-1) \n",
    "\n",
    "        out = self.layer1(flattened_data)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        out = self.layer2(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        out = self.layer3(out)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, data):\n",
    "        logits = self.logits(data)\n",
    "        return self.softmax(logits)\n",
    "    \n",
    "model = Model() # Initialize model\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modellen vår implementerer to metoder. `logits` gir unnormaliserte verdier og vil brukes under trening. `forward` bruker `softmax` til å normalisere output fra `logits`, og tas i bruk under _inference_.\n",
    "\n",
    "Vi tester modellen på sifferet vi visualiserte tidligere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data_sample.shape)\n",
    "test_input = data_sample.unsqueeze(0) # add the batch dimension, even though it is only one sample\n",
    "print(test_input.shape)\n",
    "test_input = test_input.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0802, 0.1380, 0.1177, 0.0756, 0.0949, 0.1030, 0.1175, 0.0866, 0.0988,\n",
      "         0.0877]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "The untrained model predicts the digit to be 1\n"
     ]
    }
   ],
   "source": [
    "out = model.forward(test_input)\n",
    "print(out)\n",
    "print(f\"The untrained model predicts the digit to be {out.argmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening\n",
    "Vi ser fra resultatene at modellen ikke er i stand til å avgjøre hvilket siffer inputten var. \n",
    "\n",
    "Vi har modellen og datasettet. Da er det to viktige ting som mangler for å kunne oppnå en fungerende modell. \n",
    "- **En loss-funksjon som definerer objectivet**\n",
    "    - Vi ønsker at modellen skal gi høyest sannsynlighet på det rette sifret.\n",
    "    \n",
    "- **En algoritme som utfører gradient descent, altså selve maskinlæringen**\n",
    "    - Denne algoritmen skal ta utgangspunkt i losset for å optimere parametrene.\n",
    "    - Man implementerer slike aldri selv, og finner dem heller gjennom `torch.optim`.\n",
    "\n",
    "#### Loss-funksjoner\n",
    "Loss-funksjoner sammenlikner en prediction og et target. Prediction er output fra modellen, og target er fasiten vi vet fra datasettet. \n",
    "Den enkleste er _Mean Squared Error (MSE)_, som kalkulerer gjennomsnittlig kvadrert avvik mellom tilsvarende elementer i hver tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(0.1707)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(size=(3, 4, 5))\n",
    "b = torch.zeros(size=(3, 4, 5))\n",
    "loss = F.mse_loss(a, b)\n",
    "print(loss)\n",
    "\n",
    "a = torch.rand(size=(3, 4, 5))\n",
    "b = torch.rand(size=(3, 4, 5))\n",
    "loss = F.mse_loss(a, b)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "Helt fram til beregningen av et loss konstrueres det en såkalt _computational graph_. Denne kan propageres bakover for å beregne gradienter (derav det kjente navnet [backpropagation](https://simple.wikipedia.org/wiki/Backpropagation) som brukes i alle nevrale nettverk idag).\n",
    "- Fra matematikken vet vi at gradienter peker i retning hvor verdien øker mest i en flervariabel kurve. Beveger vi oss i motsatt retning vil verdien minske.\n",
    "- Learning rate (en skalar verdi) styrer hvor store stegene er. \n",
    "\n",
    "Figuren under viser loss-funksjonen kalkulert for forskjellige verdier av parametrene $\\theta_1$ og $\\theta_2$ til en modell. \n",
    "\n",
    "[<img src=\"https://zitaoshen.rbind.io/project/optimization/1-min-of-machine-learning-gradient-decent/featured_hubf6ae7b9a0510d717632b017746fdfc1_374655_720x0_resize_lanczos_2.png\">](https://zitaoshen.rbind.io/project/optimization/1-min-of-machine-learning-gradient-decent/)\n",
    "\n",
    "<!-- Oppdatering av vektene gjøres da slikt:\n",
    "\n",
    "$\\boldsymbol{w} \\leftarrow \\underbrace{\\alpha}_{\\text{learning rate}}$ -->\n",
    "\n",
    "Selv om konkrete regler finnes for beregning av gradientene til vektene, er det svært kronglete å gjøre manuelt. Derfor takker vi AI-gudene for bibliotek som PyTorch og Tensorflow som gjør denne prosessen så og si automatisk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tilbake til treningen av en siffer-gjenkjenneren\n",
    "Til loss-funksjonen bruker vi `nn.CrossEntropyLoss`. Den sammenlikner to sannsynlighetsfordelinger. Matematikken bak er ikke så viktig.\n",
    "\n",
    "Til gradient descent bruker vi bruker Adam, som står for _Adaptive Moment Estimation_. Her er heller ikke matematikken viktig. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1875/1875 [00:06<00:00, 279.54batch/s]\n",
      "Epoch 1: 100%|██████████| 1875/1875 [00:06<00:00, 289.26batch/s]\n",
      "Epoch 2: 100%|██████████| 1875/1875 [00:06<00:00, 297.38batch/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm # Progress bar\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm(train_loader, unit=\"batch\") as pbar:\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        for i, (data, labels) in enumerate(pbar):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model.logits(data) # Query model for predictions\n",
    "            loss = loss_fn(pred, labels)\n",
    "            \n",
    "            loss.backward() # Propagate the computational graph and calculate gradients\n",
    "            optimizer.step() # Uses the calculated gradients on the registered parameters to perform an update\n",
    "            optimizer.zero_grad() # Remove the gradients\n",
    "\n",
    "            # pbar.set_postfix(loss=loss.cpu().item()) if i%40 == 0 else None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing av modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model on the test set is 0.9508999586105347\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "correct = 0\n",
    "for data, labels in test_loader:\n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    pred = model.forward(data)\n",
    "    correct += torch.sum(pred.argmax(dim=1) == labels)\n",
    "accuracy = correct/len(test_dataset)\n",
    "\n",
    "print(f\"The accuracy of the model on the test set is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.8547e-05, 2.5230e-05, 3.9246e-05, 4.9196e-03, 5.8162e-02, 1.5251e-03,\n",
      "         1.4888e-04, 8.1573e-03, 1.7281e-02, 9.0965e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "The trained model predicts the digit to be 9\n"
     ]
    }
   ],
   "source": [
    "out = model.forward(test_input)\n",
    "print(out)\n",
    "print(f\"The trained model predicts the digit to be {out.argmax()}\")\n",
    "# plt.bar(torch.linspace(0, 9, 10).numpy(), out[0].cpu().detach())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
